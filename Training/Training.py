import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras import regularizers
import matplotlib.pyplot as plt
import argparse
import joblib
import os


# === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Ñ–ª–∞–≥–∏ ===
def parse_arguments():
    parser = argparse.ArgumentParser(description='Anime Rating Prediction Model')
    parser.add_argument('--mode', choices=['train', 'test', 'both'], default='both',
                        help='–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: train (—Ç–æ–ª—å–∫–æ –æ–±—É—á–µ–Ω–∏–µ), test (—Ç–æ–ª—å–∫–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ), both (–æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)')
    parser.add_argument('--model-path', default='saved_model/anime_rating_model_improved',
                        help='–ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏')
    parser.add_argument('--epochs', type=int, default=5,
                        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--batch-size', type=int, default=128,
                        help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞')
    parser.add_argument('--data-limit', type=int, default=5000000,
                        help='–õ–∏–º–∏—Ç —Å—Ç—Ä–æ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ ratings.csv')
    parser.add_argument('--plot', action='store_true',
                        help='–ü–æ–∫–∞–∑–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--verbose', type=int, default=1,
                        help='–£—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—ã–≤–æ–¥–∞ (0, 1, 2)')

    return parser.parse_args()


# === –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö ===
def load_and_prepare_data(data_limit):
    print("=== –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===")
    anime_df = pd.read_csv("out/anime.csv")
    ratings_df = pd.read_csv("out/rating.csv").head(data_limit)

    merged_df = pd.merge(ratings_df, anime_df, on="anime_id")
    merged_df['genres'] = merged_df['genres'].fillna('').apply(lambda g: g.split('|'))

    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {merged_df.shape}")
    print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (rating_x):")
    print(merged_df['rating_x'].describe())
    print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    print(merged_df[['episodes', 'rating_y', 'members']].describe())

    return merged_df


# === –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===
def prepare_features(merged_df, mlb=None, scaler=None, is_training=True):
    print("=== –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===")

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∂–∞–Ω—Ä–æ–≤
    if is_training:
        mlb = MultiLabelBinarizer()
        genre_features = pd.DataFrame(mlb.fit_transform(merged_df['genres']), columns=mlb.classes_)
    else:
        if mlb is None:
            raise ValueError("MultiLabelBinarizer –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        genre_features = pd.DataFrame(mlb.transform(merged_df['genres']), columns=mlb.classes_)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∏–ø–æ–≤
    type_dummies = pd.get_dummies(merged_df['type'].fillna('Unknown'))

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    numeric_cols = ['episodes', 'rating_y', 'members']
    numeric_data = merged_df[numeric_cols].fillna(0)

    if is_training:
        scaler = RobustScaler()
        numeric_features_scaled = pd.DataFrame(
            scaler.fit_transform(numeric_data),
            columns=[f"{col}_scaled" for col in numeric_cols]
        )
    else:
        if scaler is None:
            raise ValueError("Scaler –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        numeric_features_scaled = pd.DataFrame(
            scaler.transform(numeric_data),
            columns=[f"{col}_scaled" for col in numeric_cols]
        )

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
    features = pd.concat([
        genre_features.reset_index(drop=True),
        type_dummies.reset_index(drop=True),
        numeric_features_scaled.reset_index(drop=True)
    ], axis=1)

    target = merged_df['rating_x'].reset_index(drop=True)

    print(f"–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features.shape}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∂–∞–Ω—Ä–æ–≤: {len(mlb.classes_)}")

    return features, target, mlb, scaler


# === –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
def create_model(input_dim, dropout_rate=0.3):
    model = models.Sequential([
        layers.Input(shape=(input_dim,)),

        # –ü–µ—Ä–≤—ã–π –±–ª–æ–∫
        layers.Dense(256, activation='relu',
                     kernel_regularizer=regularizers.l2(0.001),
                     kernel_initializer='he_normal'),
        layers.BatchNormalization(),
        layers.Dropout(dropout_rate),

        # –í—Ç–æ—Ä–æ–π –±–ª–æ–∫
        layers.Dense(128, activation='relu',
                     kernel_regularizer=regularizers.l2(0.001),
                     kernel_initializer='he_normal'),
        layers.BatchNormalization(),
        layers.Dropout(dropout_rate),

        # –¢—Ä–µ—Ç–∏–π –±–ª–æ–∫
        layers.Dense(64, activation='relu',
                     kernel_regularizer=regularizers.l2(0.001),
                     kernel_initializer='he_normal'),
        layers.BatchNormalization(),
        layers.Dropout(dropout_rate / 2),

        # –ß–µ—Ç–≤–µ—Ä—Ç—ã–π –±–ª–æ–∫
        layers.Dense(32, activation='relu',
                     kernel_initializer='he_normal'),
        layers.Dropout(dropout_rate / 2),

        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        layers.Dense(1, kernel_initializer='normal')
    ])
    return model


# === –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
def train_model(X_train, y_train, epochs, batch_size, verbose):
    print("\n=== –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∫–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ ===")
    model = create_model(X_train.shape[1])

    # –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–æ–º–ø–∏–ª—è—Ü–∏—è —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º learning rate
    initial_learning_rate = 0.001
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate,
        decay_steps=1000,
        decay_rate=0.9,
        staircase=True
    )

    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
    model.compile(
        optimizer=optimizer,
        loss='huber',  # –ë–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤–∞—è –∫ –≤—ã–±—Ä–æ—Å–∞–º —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        metrics=['mae', 'mse']
    )

    if verbose > 0:
        print(model.summary())

    # –ö–æ–ª–±—ç–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=verbose
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-7,
            verbose=verbose
        ),
        tf.keras.callbacks.ModelCheckpoint(
            'best_model.h5',
            save_best_only=True,
            monitor='val_loss',
            verbose=verbose
        )
    ]

    print("\n=== –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è ===")
    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_split=0.15,
        callbacks=callbacks,
        verbose=verbose
    )

    return model, history


# === –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ ===
def load_saved_model(model_path):
    print(f"\n=== –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {model_path} ===")
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {model_path}")

    model = tf.keras.models.load_model(model_path)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
    scaler_path = "saved_model/scaler.pkl"
    mlb_path = "saved_model/mlb.pkl"

    if not os.path.exists(scaler_path) or not os.path.exists(mlb_path):
        raise FileNotFoundError("–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã (scaler.pkl –∏–ª–∏ mlb.pkl) –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

    scaler = joblib.load(scaler_path)
    mlb = joblib.load(mlb_path)

    print("–ú–æ–¥–µ–ª—å –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    return model, scaler, mlb


# === –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
def test_model(model, X_test, y_test, verbose):
    print("\n=== –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ ===")
    results = model.evaluate(X_test, y_test, verbose=0)
    print(f"Test loss: {results[0]:.4f}")
    print(f"MAE: {results[1]:.4f}")
    print(f"MSE: {results[2]:.4f}")

    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    predictions = model.predict(X_test, verbose=0)
    residuals = y_test - predictions.flatten()

    print(f"\n=== –ê–Ω–∞–ª–∏–∑ –æ—Å—Ç–∞—Ç–∫–æ–≤ ===")
    print(f"–°—Ä–µ–¥–Ω–µ–µ –æ—Å—Ç–∞—Ç–∫–æ–≤: {np.mean(residuals):.4f}")
    print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤: {np.std(residuals):.4f}")
    print(f"–ú–µ–¥–∏–∞–Ω–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤: {np.median(residuals):.4f}")

    # –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    if verbose > 0:
        print("\n=== –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ===")
        test_indices = np.random.choice(len(X_test), min(10, len(X_test)), replace=False)

        for i, idx in enumerate(test_indices):
            pred = predictions[idx][0]
            actual = y_test.iloc[idx] if hasattr(y_test, 'iloc') else y_test[idx]
            error = abs(pred - actual)
            print(f"[{i + 1}] Predicted: {pred:.2f}, Actual: {actual:.2f}, Error: {error:.2f}")

    return predictions


# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
def save_model(model, scaler, mlb, model_path):
    print(f"\n=== –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ {model_path} ===")
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    os.makedirs("saved_model", exist_ok=True)

    model.save(model_path)
    joblib.dump(scaler, "saved_model/scaler.pkl")
    joblib.dump(mlb, "saved_model/mlb.pkl")
    print("–ú–æ–¥–µ–ª—å –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")


# === –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è ===
def plot_training_history(history):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    # Loss
    ax1.plot(history.history['loss'], label='Training Loss')
    ax1.plot(history.history['val_loss'], label='Validation Loss')
    ax1.set_title('Model Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()

    # MAE
    ax2.plot(history.history['mae'], label='Training MAE')
    ax2.plot(history.history['val_mae'], label='Validation MAE')
    ax2.set_title('Model MAE')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('MAE')
    ax2.legend()

    plt.tight_layout()
    plt.show()


# === –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ===
def main():
    args = parse_arguments()

    print(f"=== –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: {args.mode.upper()} ===")

    # –û–±—â–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    model = None
    scaler = None
    mlb = None
    history = None

    if args.mode in ['train', 'both']:
        # === –†–ï–ñ–ò–ú –û–ë–£–ß–ï–ù–ò–Ø ===
        print("\nüî• –ó–ê–ü–£–°–ö –†–ï–ñ–ò–ú–ê –û–ë–£–ß–ï–ù–ò–Ø")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        merged_df = load_and_prepare_data(args.data_limit)
        features, target, mlb, scaler = prepare_features(merged_df, is_training=True)

        # –î–µ–ª–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∫–∏
        X_train, X_test, y_train, y_test = train_test_split(
            features, target, test_size=0.2, random_state=42,
            stratify=pd.cut(target, bins=5)
        )

        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ float32
        X_train = X_train.astype(np.float32)
        X_test = X_test.astype(np.float32)
        y_train = y_train.astype(np.float32)
        y_test = y_test.astype(np.float32)

        # –û–±—É—á–µ–Ω–∏–µ
        model, history = train_model(X_train, y_train, args.epochs, args.batch_size, args.verbose)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        save_model(model, scaler, mlb, args.model_path)

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
        if args.mode == 'both':
            test_model(model, X_test, y_test, args.verbose)

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        if args.plot and history:
            plot_training_history(history)

    elif args.mode == 'test':
        # === –†–ï–ñ–ò–ú –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ===
        print("\nüß™ –ó–ê–ü–£–°–ö –†–ï–ñ–ò–ú–ê –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        model, scaler, mlb = load_saved_model(args.model_path)

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        merged_df = load_and_prepare_data(args.data_limit)
        features, target, _, _ = prepare_features(merged_df, mlb, scaler, is_training=False)

        # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å
        X_test = features.astype(np.float32)
        y_test = target.astype(np.float32)

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        test_model(model, X_test, y_test, args.verbose)

    print(f"\n‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤ —Ä–µ–∂–∏–º–µ '{args.mode}' –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")


if __name__ == "__main__":
    main()